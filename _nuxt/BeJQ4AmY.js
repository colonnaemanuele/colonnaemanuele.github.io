import{d as _,e as y,f as w,c as x,b as S,a as f,V as C}from"./DVI1745C.js";import{bb as V,d as v,r as c,o as b,I as d,y as r,C as a,B as e,D as l,A as s,z as u,$ as A,a0 as k,aM as I,J as L,K as z,_ as T,L as F,x as G,P as D,Q as q,av as N}from"./F9BUXMjc.js";const E=V("v-spacer","div","VSpacer"),g=48,M=v({__name:"Card",props:{publication:{}},setup(t){const i=c(!1),n=c(null),o=c(!1);b(()=>{n.value&&(o.value=n.value.scrollHeight>g)});const p=()=>{i.value=!i.value};return b(()=>{n.value&&(o.value=n.value.scrollHeight>g)}),(m,h)=>(r(),d(S,{class:"mb-6 full-width",elevation:"2"},{default:a(()=>[e(_,{class:"text-h6"},{default:a(()=>[l(s(t.publication.title),1)]),_:1}),e(y,null,{default:a(()=>[l(s(t.publication.authors),1)]),_:1}),e(w,null,{default:a(()=>[u("p",null,[u("strong",null,s(t.publication.journal),1),l(", "+s(t.publication.year),1)]),u("p",{ref_key:"abstractRef",ref:n,class:k(["mt-2",{"truncate-desc":!i.value}]),style:A({maxHeight:i.value?"none":`${g}px`})},s(t.publication.abstract),7)]),_:1}),e(x,null,{default:a(()=>[e(E),o.value?(r(),d(L,{key:0,icon:"",onClick:p},{default:a(()=>[e(z,null,{default:a(()=>[l(s(i.value?"mdi-chevron-up":"mdi-chevron-down"),1)]),_:1})]),_:1})):I("",!0)]),_:1})]),_:1}))}}),P=Object.assign(T(M,[["__scopeId","data-v-475d0db2"]]),{__name:"PublicationCard"}),O=v({__name:"publications",props:{publication:{}},setup(t){const i=c([{title:"Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts",authors:"P. De Marinis, N. Fanelli, R. Scaringi, E. Colonna, G. Fiameni, G. Vessio, G. Castellano",journal:"European Conference on Artificial Intelligence (ECAI), 2025",year:"2025",doi:"https://doi.org/10.3233/FAIA251289",url:"https://ebooks.iospress.nl/doi/10.3233/FAIA251289",abstract:`We present Label Anything, an innovative neural network architecture designed for few-shot semantic segmentation (FSS) that demonstrates remarkable generalizability across multiple classes with minimal examples required per class. Diverging from traditional FSS methods that predominantly rely on masks for annotating support images, Label Anything introduces varied visual prompts -- points, bounding boxes, and masks -- thereby enhancing the framework's versatility and adaptability. Unique to our approach, Label Anything is engineered for end-to-end training across multi-class FSS scenarios, efficiently learning from diverse support set configurations without retraining. This approach enables a "universal" application to various FSS challenges, ranging from -way -shot to complex -way -shot configurations while remaining agnostic to the specific number of class examples. This innovative training strategy reduces computational requirements and substantially improves the model's adaptability and generalization across diverse segmentation tasks. Our comprehensive experimental validation, particularly achieving state-of-the-art results on the COCO- benchmark, underscores Label Anything's robust generalization and flexibility. The source code is publicly available at: https://github.com/pasqualedem/LabelAnything.`},{title:"Towards Italian Sign Language Generation for digital humans",authors:"E. Colonna, A. Arezzo, D. Roberto, D. Landi, F. Vitulano, G. Vessio, G. Castellano",journal:"Eight Workshop on Natural Language for Artificial Intelligence (NL4AI 2024) @AIxIA 2024",year:"2024",url:"https://www.researchgate.net/profile/Gennaro-Vessio/publication/387398556_Towards_Italian_Sign_Language_Generation_for_digital_humans/links/676bc637fb9aff6eaaebc134/Towards-Italian-Sign-Language-Generation-for-digital-humans.pdf",abstract:"In the rapidly evolving field of human-computer interaction, the need for inclusive and accessible communication methods has become increasingly vital. This paper introduces an early exploration of Text-to-LIS, a new model designed to generate contextually accurate Italian Sign Language (LIS) gestures for digital humans. Our approach addresses the importance of non-verbal communication in virtual environments, focusing on enhancing interaction for the deaf and hard-of-hearing community. The core contribution of this work is developing an iterative framework that leverages a comprehensive multimodal dataset, integrating textual and audio inputs with visual data. Utilizing state-of-the-art deep learning algorithms and advanced human pose estimation techniques, the framework enables the progressive refinement of generated gestures, ensuring realism and contextual relevance. The potential applications of the Text-to-LIS model are wide-ranging, from improving accessibility in digital environments to supporting educational tools and promoting LIS in the digital age. The code is publicly available at: https://github.com/CarpiDiem98/text-to-lis/."},{title:"Costellazioni tipografiche-typographical costellations 15 - Tipografia Machine-Kern 0.3",authors:"E. Colonna, M. Colonna",journal:"MD JOURNAL",year:"2023",pages:"78-87",url:"https://www.researchgate.net/profile/Gennaro-Vessio/publication/387398556_Towards_Italian_Sign_Language_Generation_for_digital_humans/links/676bc637fb9aff6eaaebc134/Towards-Italian-Sign-Language-Generation-for-digital-humans.pdf",abstract:"Può l’intelligenza artificiale migliorare il lavoro di progettazione di caratteri tipografici digitali, ottimizzando uno degli aspetti più spinosi della sua realizzazione, ossia quello del kerning? Sono stati sperimentati due approcci: il primo basato su sistemi di machine learning con variabili tipografiche e rappresentazioni matematiche; il secondo addestrando modelli di deep learning somministrando matrici di pixel ad un sistema di relazioni logico-percettive. Valutando i risultati di questa prima sperimentazione risulta che il modello basato su deep learning abbia generato un errore più basso rispetto a quello di machine learning, impiegando però, risorse e tempo di apprendimento di gran lunga superiore."}]);return(n,o)=>{const p=P;return r(),d(F,null,{default:a(()=>[e(f,null,{default:a(()=>[e(C,{justify:"center",class:"mt-4"},{default:a(()=>[e(f,{cols:"12",md:"9"},{default:a(()=>[o[0]||(o[0]=u("h2",{class:"text-h4 text-center mb-6"},"Publications",-1)),(r(!0),G(D,null,q(N(i),(m,h)=>(r(),d(p,{key:h,publication:m},null,8,["publication"]))),128))]),_:1})]),_:1})]),_:1})]),_:1})}}});export{O as default};
